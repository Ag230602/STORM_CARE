{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Preparation for Recovery Modeling (DFGCN-style)\n",
        "This notebook converts your CSV inputs into model-ready tensors for ST-GNN / Temporal Transformer recovery forecasting.\n",
        "\n",
        "**Inputs:** shelters.csv, hospitals.csv, schools.csv, vulnerability_grid.csv\n",
        "**Optional:** weather_ensemble.csv, recovery_labels.csv\n",
        "\n",
        "**Outputs (saved to OUT_DIR):** nodes.csv, edges.csv, X_static.npy, edge_index.npy, edge_weight.npy, X_dynamic.npy (if weather provided), Y.npy (if labels provided), node_ids.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import BallTree, NearestNeighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e218214d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: geopandas in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: fiona in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.10.1)\n",
            "Requirement already satisfied: pyogrio in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.22 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geopandas) (2.2.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geopandas) (2.2.3)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geopandas) (3.7.0)\n",
            "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geopandas) (2.0.6)\n",
            "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fiona) (24.3.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fiona) (2024.12.14)\n",
            "Requirement already satisfied: click~=8.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fiona) (8.1.8)\n",
            "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fiona) (1.1.1.2)\n",
            "Requirement already satisfied: cligj>=0.5 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fiona) (0.7.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from click~=8.0->fiona) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adrija\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\adrija\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install geopandas fiona pyogrio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85e8f404",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers found:\n",
            "['SVI2022_FLORIDA_tract' 'MultiPolygon']\n",
            "Using layer: SVI2022_FLORIDA_tract\n",
            "Rows: 5122\n",
            "Columns: ['ST', 'STATE', 'ST_ABBR', 'STCNTY', 'COUNTY', 'FIPS', 'LOCATION', 'AREA_SQMI', 'E_TOTPOP', 'M_TOTPOP', 'E_HU', 'M_HU', 'E_HH', 'M_HH', 'E_POV150', 'M_POV150', 'E_UNEMP', 'M_UNEMP', 'E_HBURD', 'M_HBURD', 'E_NOHSDP', 'M_NOHSDP', 'E_UNINSUR', 'M_UNINSUR', 'E_AGE65', 'M_AGE65', 'E_AGE17', 'M_AGE17', 'E_DISABL', 'M_DISABL', 'E_SNGPNT', 'M_SNGPNT', 'E_LIMENG', 'M_LIMENG', 'E_MINRTY', 'M_MINRTY', 'E_MUNIT', 'M_MUNIT', 'E_MOBILE', 'M_MOBILE', 'E_CROWD', 'M_CROWD', 'E_NOVEH', 'M_NOVEH', 'E_GROUPQ', 'M_GROUPQ', 'EP_POV150', 'MP_POV150', 'EP_UNEMP', 'MP_UNEMP', 'EP_HBURD', 'MP_HBURD', 'EP_NOHSDP', 'MP_NOHSDP', 'EP_UNINSUR', 'MP_UNINSUR', 'EP_AGE65', 'MP_AGE65', 'EP_AGE17', 'MP_AGE17', 'EP_DISABL', 'MP_DISABL', 'EP_SNGPNT', 'MP_SNGPNT', 'EP_LIMENG', 'MP_LIMENG', 'EP_MINRTY', 'MP_MINRTY', 'EP_MUNIT', 'MP_MUNIT', 'EP_MOBILE', 'MP_MOBILE', 'EP_CROWD', 'MP_CROWD', 'EP_NOVEH', 'MP_NOVEH', 'EP_GROUPQ', 'MP_GROUPQ', 'EPL_POV150', 'EPL_UNEMP', 'EPL_HBURD', 'EPL_NOHSDP', 'EPL_UNINSUR', 'SPL_THEME1', 'RPL_THEME1', 'EPL_AGE65', 'EPL_AGE17', 'EPL_DISABL', 'EPL_SNGPNT', 'EPL_LIMENG', 'SPL_THEME2', 'RPL_THEME2', 'EPL_MINRTY', 'SPL_THEME3', 'RPL_THEME3', 'EPL_MUNIT', 'EPL_MOBILE', 'EPL_CROWD', 'EPL_NOVEH', 'EPL_GROUPQ', 'SPL_THEME4', 'RPL_THEME4', 'SPL_THEMES', 'RPL_THEMES', 'F_POV150', 'F_UNEMP', 'F_HBURD', 'F_NOHSDP', 'F_UNINSUR', 'F_THEME1', 'F_AGE65', 'F_AGE17', 'F_DISABL', 'F_SNGPNT', 'F_LIMENG', 'F_THEME2', 'F_MINRTY', 'F_THEME3', 'F_MUNIT', 'F_MOBILE', 'F_CROWD', 'F_NOVEH', 'F_GROUPQ', 'F_THEME4', 'F_TOTAL', 'E_DAYPOP', 'E_NOINT', 'M_NOINT', 'E_AFAM', 'M_AFAM', 'E_HISP', 'M_HISP', 'E_ASIAN', 'M_ASIAN', 'E_AIAN', 'M_AIAN', 'E_NHPI', 'M_NHPI', 'E_TWOMORE', 'M_TWOMORE', 'E_OTHERRACE', 'M_OTHERRACE', 'EP_NOINT', 'MP_NOINT', 'EP_AFAM', 'MP_AFAM', 'EP_HISP', 'MP_HISP', 'EP_ASIAN', 'MP_ASIAN', 'EP_AIAN', 'MP_AIAN', 'EP_NHPI', 'MP_NHPI', 'EP_TWOMORE', 'MP_TWOMORE', 'EP_OTHERRACE', 'MP_OTHERRACE', 'Shape_Length', 'Shape_Area', 'geometry']\n",
            "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid.csv\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pyogrio\n",
        "import os\n",
        "\n",
        "GDB_PATH = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\SVI2022_FLORIDA_tract.gdb\"\n",
        "OUT_CSV  = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\vulnerability_grid.csv\"\n",
        "\n",
        "# List layers using pyogrio (this is the key fix)\n",
        "layers = pyogrio.list_layers(GDB_PATH)\n",
        "print(\"Layers found:\")\n",
        "for l in layers:\n",
        "    print(l)\n",
        "\n",
        "# Usually the first layer contains tract-level SVI data\n",
        "layer_name = layers[0][0]\n",
        "print(\"Using layer:\", layer_name)\n",
        "\n",
        "# Read the layer\n",
        "gdf = gpd.read_file(GDB_PATH, layer=layer_name)\n",
        "\n",
        "print(\"Rows:\", len(gdf))\n",
        "print(\"Columns:\", list(gdf.columns))\n",
        "\n",
        "# Drop geometry (we only need attributes for modeling)\n",
        "df = gdf.drop(columns=\"geometry\")\n",
        "\n",
        "# Save to CSV\n",
        "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "\n",
        "print(\"Saved:\", OUT_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cbf7a871",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved cleaned vulnerability grid: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid_clean.csv\n",
            "Rows: 5122 | Feature columns: ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES']\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# SVI -> vulnerability_grid.csv -> node-level SVI features (direct code)\n",
        "# Works with: data/raw/vulnerability/vulnerability_grid.csv\n",
        "# Outputs:\n",
        "#   1) data/processed/vulnerability_grid_clean.csv\n",
        "#   2) (optional) merges into your nodes dataframe as node_svi_features\n",
        "# ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- paths ----\n",
        "VULN_GRID_CSV_IN  = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\vulnerability_grid.csv\"      # produced from .gdb using pyogrio\n",
        "VULN_GRID_CSV_OUT = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\vulnerability_grid_clean.csv\"\n",
        "\n",
        "# If you already have nodes with lat/lon (recommended), set this:\n",
        "# Example: \"recovery_dataset_out/nodes.csv\" or \"data/processed/nodes.csv\"\n",
        "NODES_CSV = None  # e.g., \"recovery_dataset_out/nodes.csv\"\n",
        "NODES_OUT = None  # e.g., \"recovery_dataset_out/nodes_with_svi.csv\"\n",
        "\n",
        "# ---- chosen SVI features (standard, reviewer-safe) ----\n",
        "SVI_FEATURES = [\"RPL_THEME1\", \"RPL_THEME2\", \"RPL_THEME3\", \"RPL_THEME4\", \"RPL_THEMES\"]\n",
        "\n",
        "# ---- helper: robust numeric conversion ----\n",
        "def to_num(series: pd.Series) -> pd.Series:\n",
        "    return pd.to_numeric(series, errors=\"coerce\")\n",
        "\n",
        "# ---- 1) Load and clean vulnerability grid ----\n",
        "vuln = pd.read_csv(VULN_GRID_CSV_IN, dtype=str)\n",
        "\n",
        "# Required identifier columns vary slightly by release; we only strictly need FIPS\n",
        "if \"FIPS\" not in vuln.columns:\n",
        "    raise ValueError(\"Expected 'FIPS' column in vulnerability_grid.csv. Please open the CSV and check column names.\")\n",
        "\n",
        "# Convert SVI columns to numeric and clean\n",
        "for c in SVI_FEATURES:\n",
        "    if c not in vuln.columns:\n",
        "        raise ValueError(f\"Missing SVI column '{c}'. Available columns include: {list(vuln.columns)[:30]} ...\")\n",
        "    vuln[c] = to_num(vuln[c])\n",
        "\n",
        "# Drop rows with missing key SVI values\n",
        "vuln_clean = vuln[[\"FIPS\"] + SVI_FEATURES].dropna(subset=SVI_FEATURES).copy()\n",
        "\n",
        "# Clip to [0,1] just in case (RPL_* are percentiles, should already be 0..1)\n",
        "for c in SVI_FEATURES:\n",
        "    vuln_clean[c] = vuln_clean[c].clip(0.0, 1.0)\n",
        "\n",
        "# Save cleaned vulnerability grid\n",
        "os.makedirs(os.path.dirname(VULN_GRID_CSV_OUT), exist_ok=True)\n",
        "vuln_clean.to_csv(VULN_GRID_CSV_OUT, index=False)\n",
        "print(\"✅ Saved cleaned vulnerability grid:\", VULN_GRID_CSV_OUT)\n",
        "print(\"Rows:\", len(vuln_clean), \"| Feature columns:\", SVI_FEATURES)\n",
        "\n",
        "# ---- 2) OPTIONAL: merge SVI into your nodes table (if you have tract FIPS per node)\n",
        "# This requires your nodes.csv to have either:\n",
        "#   - a column named 'FIPS', OR\n",
        "#   - a column named 'tract_fips', OR\n",
        "#   - any column you can rename to 'FIPS'\n",
        "#\n",
        "# If you do NOT have tract FIPS in nodes yet, you need a spatial join:\n",
        "#   node (lat/lon) -> tract polygon -> tract FIPS\n",
        "# That step requires a tract boundary shapefile/geojson. Tell me if you want that.\n",
        "# ----\n",
        "if NODES_CSV is not None and NODES_OUT is not None:\n",
        "    nodes = pd.read_csv(NODES_CSV)\n",
        "\n",
        "    # Find a plausible join key in nodes\n",
        "    join_key = None\n",
        "    for cand in [\"FIPS\", \"tract_fips\", \"TRACT_FIPS\", \"GEOID\", \"geoid\"]:\n",
        "        if cand in nodes.columns:\n",
        "            join_key = cand\n",
        "            break\n",
        "\n",
        "    if join_key is None:\n",
        "        raise ValueError(\n",
        "            \"Nodes CSV does not contain a tract id column (FIPS/GEOID). \"\n",
        "            \"Add tract FIPS to nodes first (spatial join), then re-run this merge.\"\n",
        "        )\n",
        "\n",
        "    # Normalize to string 11-digit tract FIPS where possible\n",
        "    nodes[\"__FIPS__\"] = nodes[join_key].astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.zfill(11)\n",
        "    vuln_clean[\"__FIPS__\"] = vuln_clean[\"FIPS\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True).str.zfill(11)\n",
        "\n",
        "    merged = nodes.merge(vuln_clean[[\"__FIPS__\"] + SVI_FEATURES], on=\"__FIPS__\", how=\"left\")\n",
        "\n",
        "    # If any missing, fill with median (reasonable default)\n",
        "    for c in SVI_FEATURES:\n",
        "        if merged[c].isna().any():\n",
        "            med = float(np.nanmedian(merged[c].values))\n",
        "            merged[c] = merged[c].fillna(med)\n",
        "\n",
        "    merged = merged.drop(columns=[\"__FIPS__\"])\n",
        "\n",
        "    os.makedirs(os.path.dirname(NODES_OUT), exist_ok=True)\n",
        "    merged.to_csv(NODES_OUT, index=False)\n",
        "    print(\"✅ Saved nodes with SVI features:\", NODES_OUT)\n",
        "    print(\"Merged node rows:\", len(merged))\n",
        "\n",
        "# ---- 3) Quick tensor export (optional): save SVI feature matrix for GNN\n",
        "# If you have a nodes_with_svi.csv already and want an X_static.npy:\n",
        "#   set NODES_WITH_SVI_CSV and run this block\n",
        "# ----\n",
        "NODES_WITH_SVI_CSV = None  # e.g., \"recovery_dataset_out/nodes_with_svi.csv\"\n",
        "X_STATIC_OUT = None        # e.g., \"recovery_dataset_out/X_svi.npy\"\n",
        "\n",
        "if NODES_WITH_SVI_CSV is not None and X_STATIC_OUT is not None:\n",
        "    ndf = pd.read_csv(NODES_WITH_SVI_CSV)\n",
        "    X_svi = ndf[SVI_FEATURES].astype(float).to_numpy(dtype=np.float32)\n",
        "    os.makedirs(os.path.dirname(X_STATIC_OUT), exist_ok=True)\n",
        "    np.save(X_STATIC_OUT, X_svi)\n",
        "    print(\"✅ Saved SVI feature matrix:\", X_STATIC_OUT, \"| shape:\", X_svi.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Set paths and output folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "240318de",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "weather_df = None\n",
        "if WEATHER_ENSEMBLE_CSV and os.path.exists(WEATHER_ENSEMBLE_CSV):\n",
        "    weather_df = pd.read_csv(WEATHER_ENSEMBLE_CSV)\n",
        "\n",
        "labels_df = None\n",
        "if RECOVERY_LABELS_CSV and os.path.exists(RECOVERY_LABELS_CSV):\n",
        "    labels_df = pd.read_csv(RECOVERY_LABELS_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ed934e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\weather_ensemble.csv shape: (48000, 6)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "OUT = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\weather_ensemble.csv\"\n",
        "T = 24          # time steps\n",
        "N = 200         # nodes (set to your node count)\n",
        "E = 10          # ensemble members\n",
        "\n",
        "rows = []\n",
        "for t in range(T):\n",
        "    for node_id in range(N):\n",
        "        base_wind = 10 + 20*np.sin(t/6) + np.random.randn()*1.0\n",
        "        base_prec = max(0, 5*np.cos(t/4) + np.random.randn()*0.5)\n",
        "        base_mslp = 1010 - 20*np.sin(t/8) + np.random.randn()*0.8\n",
        "        for ens in range(E):\n",
        "            rows.append([t, node_id, ens,\n",
        "                         base_wind + np.random.randn()*2,\n",
        "                         max(0, base_prec + np.random.randn()*0.4),\n",
        "                         base_mslp + np.random.randn()*1.5])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"t\",\"node_id\",\"ens\",\"wind10m\",\"precip\",\"mslp\"])\n",
        "os.makedirs(os.path.dirname(OUT), exist_ok=True)\n",
        "df.to_csv(OUT, index=False)\n",
        "print(\"Saved:\", OUT, \"shape:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3b2d4163",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using SVI layer: SVI2022_FLORIDA_tract\n",
            "✅ Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\processed\\facility_svi.csv shape: (7710, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>facility_id</th>\n",
              "      <th>facility_type</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.1097</td>\n",
              "      <td>0.6734</td>\n",
              "      <td>0.5022</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.2994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>shelter_1</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.5787</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.5254</td>\n",
              "      <td>0.8065</td>\n",
              "      <td>0.3341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>shelter_2</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.0574</td>\n",
              "      <td>0.5521</td>\n",
              "      <td>0.3959</td>\n",
              "      <td>0.5821</td>\n",
              "      <td>0.2783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shelter_3</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.5118</td>\n",
              "      <td>0.3832</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>0.8684</td>\n",
              "      <td>0.6544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shelter_4</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.0339</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.3909</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  facility_id facility_type  RPL_THEME1  RPL_THEME2  RPL_THEME3  RPL_THEME4  \\\n",
              "0   shelter_0       shelter      0.1097      0.6734      0.5022      0.3905   \n",
              "1   shelter_1       shelter      0.5787      0.0008      0.5254      0.8065   \n",
              "2   shelter_2       shelter      0.0574      0.5521      0.3959      0.5821   \n",
              "3   shelter_3       shelter      0.5118      0.3832      0.5423      0.8684   \n",
              "4   shelter_4       shelter      0.0339      0.0357      0.3909      0.2744   \n",
              "\n",
              "   RPL_THEMES  \n",
              "0      0.2994  \n",
              "1      0.3341  \n",
              "2      0.2783  \n",
              "3      0.6544  \n",
              "4      0.0519  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import pyogrio\n",
        "\n",
        "# ---- Paths (edit if needed) ----\n",
        "SHELTERS_CSV  = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\shelters.csv\"\n",
        "HOSPITALS_CSV = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\hospitals.csv\"\n",
        "SCHOOLS_CSV   = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\schools.csv\"\n",
        "\n",
        "GDB_PATH = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\SVI2022_FLORIDA_tract.gdb\"\n",
        "OUT_FAC_SVI = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\processed\\\\facility_svi.csv\"\n",
        "\n",
        "SVI_FEATURES = [\"RPL_THEME1\",\"RPL_THEME2\",\"RPL_THEME3\",\"RPL_THEME4\",\"RPL_THEMES\"]\n",
        "\n",
        "def detect_latlon(df):\n",
        "    candidates = [\n",
        "        (\"latitude\",\"longitude\"),\n",
        "        (\"lat\",\"lon\"),\n",
        "        (\"LAT\",\"LON\"),\n",
        "        (\"Latitude\",\"Longitude\"),\n",
        "        (\"Y\",\"X\"),\n",
        "        (\"y\",\"x\")\n",
        "    ]\n",
        "    for a,b in candidates:\n",
        "        if a in df.columns and b in df.columns:\n",
        "            return a,b\n",
        "    raise ValueError(f\"Could not detect lat/lon columns. Columns: {list(df.columns)}\")\n",
        "\n",
        "def load_facilities(path, facility_type):\n",
        "    df = pd.read_csv(path)\n",
        "    lat_col, lon_col = detect_latlon(df)\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "    gdf[\"facility_type\"] = facility_type\n",
        "    # create a stable id\n",
        "    gdf[\"facility_id\"] = facility_type + \"_\" + gdf.index.astype(str)\n",
        "    return gdf\n",
        "\n",
        "# Load facilities\n",
        "fac = pd.concat([\n",
        "    load_facilities(SHELTERS_CSV, \"shelter\"),\n",
        "    load_facilities(HOSPITALS_CSV, \"hospital\"),\n",
        "    load_facilities(SCHOOLS_CSV, \"school\")\n",
        "], ignore_index=True)\n",
        "\n",
        "# Load SVI tract polygons from GDB\n",
        "layers = pyogrio.list_layers(GDB_PATH)\n",
        "layer_name = layers[0][0]\n",
        "print(\"Using SVI layer:\", layer_name)\n",
        "\n",
        "tracts = gpd.read_file(GDB_PATH, layer=layer_name).to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Ensure SVI columns exist\n",
        "missing = [c for c in SVI_FEATURES if c not in tracts.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing SVI columns in tract data: {missing}\")\n",
        "\n",
        "# Spatial join facility points -> tracts\n",
        "joined = gpd.sjoin(\n",
        "    fac,\n",
        "    tracts[SVI_FEATURES + [\"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "\n",
        "# Clean / fill missing SVI\n",
        "for c in SVI_FEATURES:\n",
        "    joined[c] = pd.to_numeric(joined[c], errors=\"coerce\")\n",
        "    joined[c] = joined[c].fillna(joined[c].median()).clip(0, 1)\n",
        "\n",
        "# Save facility-level SVI\n",
        "out = joined.drop(columns=\"geometry\")[[\"facility_id\",\"facility_type\"] + SVI_FEATURES].copy()\n",
        "os.makedirs(os.path.dirname(OUT_FAC_SVI), exist_ok=True)\n",
        "out.to_csv(OUT_FAC_SVI, index=False)\n",
        "\n",
        "print(\"✅ Saved:\", OUT_FAC_SVI, \"shape:\", out.shape)\n",
        "out.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bdfec3e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\recovery_labels.csv (185040, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>facility_id</th>\n",
              "      <th>facility_type</th>\n",
              "      <th>recovery_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.464446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.457115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.456879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.475324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>shelter_0</td>\n",
              "      <td>shelter</td>\n",
              "      <td>0.516928</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   t facility_id facility_type  recovery_index\n",
              "0  0   shelter_0       shelter        0.464446\n",
              "1  1   shelter_0       shelter        0.457115\n",
              "2  2   shelter_0       shelter        0.456879\n",
              "3  3   shelter_0       shelter        0.475324\n",
              "4  4   shelter_0       shelter        0.516928"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "WEA = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\weather_ensemble.csv\"            # must exist if using this option\n",
        "FAC_SVI = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\processed\\\\facility_svi.csv\"\n",
        "OUT = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\recovery_labels.csv\"\n",
        "\n",
        "fac = pd.read_csv(FAC_SVI)[[\"facility_id\",\"facility_type\",\"RPL_THEMES\"]]\n",
        "w = pd.read_csv(WEA)\n",
        "\n",
        "# If weather is node-based, we don't have facility->node mapping.\n",
        "# For demo: aggregate to time only (statewide mean), then apply facility vulnerability.\n",
        "w_t = w.groupby(\"t\", as_index=False)[[\"wind10m\",\"precip\",\"mslp\"]].mean()\n",
        "\n",
        "# Normalize hazard\n",
        "wind_norm = (w_t[\"wind10m\"] - w_t[\"wind10m\"].min()) / (w_t[\"wind10m\"].max() - w_t[\"wind10m\"].min() + 1e-9)\n",
        "prec_norm = (w_t[\"precip\"] - w_t[\"precip\"].min()) / (w_t[\"precip\"].max() - w_t[\"precip\"].min() + 1e-9)\n",
        "damage_t = 0.6*wind_norm + 0.4*prec_norm\n",
        "\n",
        "rows = []\n",
        "T = len(w_t)\n",
        "for _, r in fac.iterrows():\n",
        "    vuln = float(np.clip(r[\"RPL_THEMES\"], 0, 1))\n",
        "    for i, t in enumerate(w_t[\"t\"].values):\n",
        "        time_factor = i / (T - 1 + 1e-9)\n",
        "        base = 1.0 - (0.7*damage_t.iloc[i] + 0.3*vuln)\n",
        "        recovery = np.clip(base + 0.6*time_factor*(1.0 - vuln), 0, 1)\n",
        "        rows.append([t, r[\"facility_id\"], r[\"facility_type\"], recovery])\n",
        "\n",
        "labels = pd.DataFrame(rows, columns=[\"t\",\"facility_id\",\"facility_type\",\"recovery_index\"])\n",
        "os.makedirs(os.path.dirname(OUT), exist_ok=True)\n",
        "labels.to_csv(OUT, index=False)\n",
        "print(\"✅ Saved:\", OUT, labels.shape)\n",
        "labels.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3bef1b5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUT_DIR: C:\\Users\\Adrija\\Downloads\\DFGCN\\recovery_dataset_out\n",
            "Shelters exists: False\n",
            "Hospitals exists: False\n",
            "Schools exists: False\n",
            "Vuln exists: False\n",
            "Weather exists: False\n",
            "Labels exists: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# --------- Use ONE base folder to avoid duplicated paths ----------\n",
        "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\"\n",
        "\n",
        "SHELTERS_CSV  = os.path.join(BASE, r\"data\\raw\\facilities\\shelters.csv\")\n",
        "HOSPITALS_CSV = os.path.join(BASE, r\"data\\raw\\facilities\\hospitals.csv\")\n",
        "SCHOOLS_CSV   = os.path.join(BASE, r\"data\\raw\\facilities\\schools.csv\")\n",
        "\n",
        "VULN_GRID_CSV = os.path.join(BASE, r\"data\\raw\\vulnerability\\vulnerability_grid.csv\")\n",
        "\n",
        "# Optional (only if the file exists)\n",
        "WEATHER_ENSEMBLE_CSV = os.path.join(BASE, r\"data\\raw\\weather_ensemble.csv\")   # t,node_id,ens,...\n",
        "RECOVERY_LABELS_CSV  = os.path.join(BASE, r\"data\\raw\\recovery_labels.csv\")    # depends on your labeling format\n",
        "\n",
        "# Output directory (MUST be defined)\n",
        "OUT_DIR = os.path.join(BASE, \"recovery_dataset_out\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "# Safety checks (recommended)\n",
        "print(\"Shelters exists:\", os.path.exists(SHELTERS_CSV))\n",
        "print(\"Hospitals exists:\", os.path.exists(HOSPITALS_CSV))\n",
        "print(\"Schools exists:\", os.path.exists(SCHOOLS_CSV))\n",
        "print(\"Vuln exists:\", os.path.exists(VULN_GRID_CSV))\n",
        "print(\"Weather exists:\", os.path.exists(WEATHER_ENSEMBLE_CSV))\n",
        "print(\"Labels exists:\", os.path.exists(RECOVERY_LABELS_CSV))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bed4f57a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOUND: shelters.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\shelters.csv\n",
            "FOUND: hospitals.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\hospitals.csv\n",
            "FOUND: schools.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\schools.csv\n",
            "FOUND: vulnerability_grid_clean.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid_clean.csv\n",
            "FOUND: weather_ensemble.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\weather_ensemble.csv\n",
            "FOUND: recovery_labels.csv -> C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\recovery_labels.csv\n",
            "\n",
            "--- Final Paths ---\n",
            "SHELTERS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\shelters.csv\n",
            "HOSPITALS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\hospitals.csv\n",
            "SCHOOLS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\schools.csv\n",
            "VULN_GRID_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid_clean.csv\n",
            "WEATHER_ENSEMBLE_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\weather_ensemble.csv\n",
            "RECOVERY_LABELS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\recovery_labels.csv\n",
            "OUT_DIR: C:\\Users\\Adrija\\Downloads\\DFGCN\\recovery_dataset_out\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\"\n",
        "\n",
        "def find_one(pattern):\n",
        "    hits = glob.glob(os.path.join(BASE, \"**\", pattern), recursive=True)\n",
        "    if not hits:\n",
        "        print(\"NOT FOUND:\", pattern)\n",
        "        return None\n",
        "    # pick shortest path (usually the correct one)\n",
        "    hits = sorted(hits, key=len)\n",
        "    print(\"FOUND:\", pattern, \"->\", hits[0])\n",
        "    return hits[0]\n",
        "\n",
        "SHELTERS_CSV  = find_one(\"shelters.csv\")\n",
        "HOSPITALS_CSV = find_one(\"hospitals.csv\")\n",
        "SCHOOLS_CSV   = find_one(\"schools.csv\")\n",
        "\n",
        "# vulnerability file (you have both)\n",
        "VULN_GRID_CSV = find_one(\"vulnerability_grid_clean.csv\") or find_one(\"vulnerability_grid.csv\")\n",
        "\n",
        "WEATHER_ENSEMBLE_CSV = find_one(\"weather_ensemble.csv\")  # optional\n",
        "\n",
        "# you already have this\n",
        "RECOVERY_LABELS_CSV = find_one(\"recovery_labels.csv\")\n",
        "\n",
        "OUT_DIR = os.path.join(BASE, \"recovery_dataset_out\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n--- Final Paths ---\")\n",
        "print(\"SHELTERS_CSV:\", SHELTERS_CSV)\n",
        "print(\"HOSPITALS_CSV:\", HOSPITALS_CSV)\n",
        "print(\"SCHOOLS_CSV:\", SCHOOLS_CSV)\n",
        "print(\"VULN_GRID_CSV:\", VULN_GRID_CSV)\n",
        "print(\"WEATHER_ENSEMBLE_CSV:\", WEATHER_ENSEMBLE_CSV)\n",
        "print(\"RECOVERY_LABELS_CSV:\", RECOVERY_LABELS_CSV)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9cc62a51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['t', 'facility_id', 'facility_type', 'recovery_index']\n",
            "   t facility_id facility_type  recovery_index\n",
            "0  0   shelter_0       shelter        0.464446\n",
            "1  1   shelter_0       shelter        0.457115\n",
            "2  2   shelter_0       shelter        0.456879\n",
            "3  3   shelter_0       shelter        0.475324\n",
            "4  4   shelter_0       shelter        0.516928\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "labels = pd.read_csv(RECOVERY_LABELS_CSV)\n",
        "print(labels.columns.tolist())\n",
        "print(labels.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load & standardize CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "061f95f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SHELTERS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\shelters.csv\n",
            "HOSPITALS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\hospitals.csv\n",
            "SCHOOLS_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\schools.csv\n",
            "VULN_GRID_CSV: C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid_clean.csv\n",
            "True C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\shelters.csv\n",
            "True C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\hospitals.csv\n",
            "True C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\facilities\\schools.csv\n",
            "True C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\vulnerability_grid_clean.csv\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "\n",
        "BASE = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\"\n",
        "\n",
        "def find_one(filename):\n",
        "    hits = glob.glob(os.path.join(BASE, \"**\", filename), recursive=True)\n",
        "    if not hits:\n",
        "        raise FileNotFoundError(f\"Could not find {filename} anywhere under {BASE}\")\n",
        "    hits = sorted(hits, key=len)\n",
        "    return hits[0]\n",
        "\n",
        "SHELTERS_CSV  = find_one(\"shelters.csv\")\n",
        "HOSPITALS_CSV = find_one(\"hospitals.csv\")\n",
        "SCHOOLS_CSV   = find_one(\"schools.csv\")\n",
        "\n",
        "# vulnerability: choose clean if present\n",
        "try:\n",
        "    VULN_GRID_CSV = find_one(\"vulnerability_grid_clean.csv\")\n",
        "except FileNotFoundError:\n",
        "    VULN_GRID_CSV = find_one(\"vulnerability_grid.csv\")\n",
        "\n",
        "print(\"SHELTERS_CSV:\", SHELTERS_CSV)\n",
        "print(\"HOSPITALS_CSV:\", HOSPITALS_CSV)\n",
        "print(\"SCHOOLS_CSV:\", SCHOOLS_CSV)\n",
        "print(\"VULN_GRID_CSV:\", VULN_GRID_CSV)\n",
        "\n",
        "# sanity check\n",
        "for p in [SHELTERS_CSV, HOSPITALS_CSV, SCHOOLS_CSV, VULN_GRID_CSV]:\n",
        "    print(os.path.exists(p), p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "78dcdc53",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vuln: (5122, 6)\n",
            "vuln: (5122, 6) shelters: (1978, 83) hospitals: (351, 63) schools: (5381, 59)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def pick_latlon_cols(df):\n",
        "    candidates = [\n",
        "        (\"lat\",\"lon\"),\n",
        "        (\"LAT\",\"LON\"),\n",
        "        (\"latitude\",\"longitude\"),\n",
        "        (\"Latitude\",\"Longitude\"),\n",
        "        (\"LATITUDE\",\"LONGITUDE\"),\n",
        "        (\"y\",\"x\"),\n",
        "        (\"Y\",\"X\"),\n",
        "    ]\n",
        "    for a,b in candidates:\n",
        "        if a in df.columns and b in df.columns:\n",
        "            return a,b\n",
        "    raise ValueError(f\"Could not detect lat/lon columns. Available columns: {list(df.columns)}\")\n",
        "\n",
        "def standardize_facility(df: pd.DataFrame, kind: str) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"kind\"] = kind\n",
        "    if \"id\" not in df.columns:\n",
        "        df[\"id\"] = [f\"{kind}_{i}\" for i in range(len(df))]\n",
        "\n",
        "    lat_col, lon_col = pick_latlon_cols(df)\n",
        "    df[\"lat\"] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
        "    df[\"lon\"] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
        "\n",
        "    return df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
        "\n",
        "def standardize_grid(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"cell_id\" not in df.columns:\n",
        "        df[\"cell_id\"] = [f\"cell_{i}\" for i in range(len(df))]\n",
        "\n",
        "    lat_col, lon_col = pick_latlon_cols(df)\n",
        "    df[\"lat\"] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
        "    df[\"lon\"] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
        "\n",
        "    return df.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
        "\n",
        "shelters  = standardize_facility(pd.read_csv(SHELTERS_CSV), \"shelter\")\n",
        "hospitals = standardize_facility(pd.read_csv(HOSPITALS_CSV), \"hospital\")\n",
        "schools   = standardize_facility(pd.read_csv(SCHOOLS_CSV), \"school\")\n",
        "# grid      = standardize_grid(pd.read_csv(VULN_GRID_CSV))\n",
        "vuln = pd.read_csv(VULN_GRID_CSV, dtype={\"FIPS\": str})\n",
        "vuln[\"FIPS\"] = vuln[\"FIPS\"].str.zfill(11)\n",
        "print(\"vuln:\", vuln.shape)\n",
        "vuln.head()\n",
        "\n",
        "\n",
        "# print(\"grid:\", grid.shape, \"shelters:\", shelters.shape, \"hospitals:\", hospitals.shape, \"schools:\", schools.shape)\n",
        "print(\n",
        "    \"vuln:\", vuln.shape,\n",
        "    \"shelters:\", shelters.shape,\n",
        "    \"hospitals:\", hospitals.shape,\n",
        "    \"schools:\", schools.shape\n",
        ")\n",
        "\n",
        "# grid.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad501a5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Aggregate facilities into grid nodes (Option A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ce3b90d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import BallTree\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "61f9b707",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using layer: SVI2022_FLORIDA_tract\n",
            "✅ grid created: (5122, 8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Adrija\\AppData\\Local\\Temp\\ipykernel_20368\\1916740782.py:20: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  cent[\"geometry\"] = cent.geometry.centroid\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIPS</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12001000201</td>\n",
              "      <td>29.654545</td>\n",
              "      <td>-82.333741</td>\n",
              "      <td>0.2704</td>\n",
              "      <td>0.5495</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.4077</td>\n",
              "      <td>0.6982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12001000202</td>\n",
              "      <td>29.646142</td>\n",
              "      <td>-82.332971</td>\n",
              "      <td>0.3791</td>\n",
              "      <td>0.7495</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.4254</td>\n",
              "      <td>0.7609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12001000301</td>\n",
              "      <td>29.666673</td>\n",
              "      <td>-82.331202</td>\n",
              "      <td>0.6544</td>\n",
              "      <td>0.5118</td>\n",
              "      <td>0.3832</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>0.8684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12001000302</td>\n",
              "      <td>29.683551</td>\n",
              "      <td>-82.330493</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.4880</td>\n",
              "      <td>0.3051</td>\n",
              "      <td>0.5136</td>\n",
              "      <td>0.4241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12001000400</td>\n",
              "      <td>29.677770</td>\n",
              "      <td>-82.307900</td>\n",
              "      <td>0.7781</td>\n",
              "      <td>0.6217</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.8108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FIPS        lat        lon  RPL_THEMES  RPL_THEME1  RPL_THEME2  \\\n",
              "0  12001000201  29.654545 -82.333741      0.2704      0.5495      0.0024   \n",
              "1  12001000202  29.646142 -82.332971      0.3791      0.7495      0.0006   \n",
              "2  12001000301  29.666673 -82.331202      0.6544      0.5118      0.3832   \n",
              "3  12001000302  29.683551 -82.330493      0.4292      0.4880      0.3051   \n",
              "4  12001000400  29.677770 -82.307900      0.7781      0.6217      0.8306   \n",
              "\n",
              "   RPL_THEME3  RPL_THEME4  \n",
              "0      0.4077      0.6982  \n",
              "1      0.4254      0.7609  \n",
              "2      0.5423      0.8684  \n",
              "3      0.5136      0.4241  \n",
              "4      0.6124      0.8108  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pyogrio\n",
        "import os\n",
        "\n",
        "GDB_PATH = r\"C:\\Users\\Adrija\\Downloads\\DFGCN\\data\\raw\\data\\raw\\vulnerability\\SVI2022_FLORIDA_tract.gdb\"\n",
        "\n",
        "layers = pyogrio.list_layers(GDB_PATH)\n",
        "layer_name = layers[0][0]\n",
        "print(\"Using layer:\", layer_name)\n",
        "\n",
        "tracts = gpd.read_file(GDB_PATH, layer=layer_name).to_crs(\"EPSG:4326\")\n",
        "\n",
        "# detect FIPS column\n",
        "possible_fips = [\"FIPS\", \"GEOID\", \"GEOID10\", \"GEOID20\"]\n",
        "fips_col = next((c for c in possible_fips if c in tracts.columns), None)\n",
        "if fips_col is None:\n",
        "    raise ValueError(f\"Could not find FIPS/GEOID in columns: {list(tracts.columns)[:50]}\")\n",
        "\n",
        "cent = tracts.copy()\n",
        "cent[\"geometry\"] = cent.geometry.centroid\n",
        "cent[\"lat\"] = cent.geometry.y\n",
        "cent[\"lon\"] = cent.geometry.x\n",
        "\n",
        "grid = cent[[fips_col, \"lat\", \"lon\", \"RPL_THEMES\", \"RPL_THEME1\",\"RPL_THEME2\",\"RPL_THEME3\",\"RPL_THEME4\"]].rename(columns={fips_col:\"FIPS\"}).copy()\n",
        "grid[\"FIPS\"] = grid[\"FIPS\"].astype(str).str.zfill(11)\n",
        "\n",
        "print(\"✅ grid created:\", grid.shape)\n",
        "grid.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92722609",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "914170b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ grid_aug: (5122, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIPS</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>shelter_count_nearby</th>\n",
              "      <th>shelter_capacity_nearby</th>\n",
              "      <th>hospital_count_nearby</th>\n",
              "      <th>hospital_beds_nearby</th>\n",
              "      <th>school_count_nearby</th>\n",
              "      <th>school_capacity_nearby</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12001000201</td>\n",
              "      <td>29.654545</td>\n",
              "      <td>-82.333741</td>\n",
              "      <td>0.2704</td>\n",
              "      <td>0.5495</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.4077</td>\n",
              "      <td>0.6982</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12001000202</td>\n",
              "      <td>29.646142</td>\n",
              "      <td>-82.332971</td>\n",
              "      <td>0.3791</td>\n",
              "      <td>0.7495</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.4254</td>\n",
              "      <td>0.7609</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12001000301</td>\n",
              "      <td>29.666673</td>\n",
              "      <td>-82.331202</td>\n",
              "      <td>0.6544</td>\n",
              "      <td>0.5118</td>\n",
              "      <td>0.3832</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>0.8684</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12001000302</td>\n",
              "      <td>29.683551</td>\n",
              "      <td>-82.330493</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.4880</td>\n",
              "      <td>0.3051</td>\n",
              "      <td>0.5136</td>\n",
              "      <td>0.4241</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12001000400</td>\n",
              "      <td>29.677770</td>\n",
              "      <td>-82.307900</td>\n",
              "      <td>0.7781</td>\n",
              "      <td>0.6217</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.8108</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FIPS        lat        lon  RPL_THEMES  RPL_THEME1  RPL_THEME2  \\\n",
              "0  12001000201  29.654545 -82.333741      0.2704      0.5495      0.0024   \n",
              "1  12001000202  29.646142 -82.332971      0.3791      0.7495      0.0006   \n",
              "2  12001000301  29.666673 -82.331202      0.6544      0.5118      0.3832   \n",
              "3  12001000302  29.683551 -82.330493      0.4292      0.4880      0.3051   \n",
              "4  12001000400  29.677770 -82.307900      0.7781      0.6217      0.8306   \n",
              "\n",
              "   RPL_THEME3  RPL_THEME4  shelter_count_nearby  shelter_capacity_nearby  \\\n",
              "0      0.4077      0.6982                    17                      0.0   \n",
              "1      0.4254      0.7609                    17                      0.0   \n",
              "2      0.5423      0.8684                    17                      0.0   \n",
              "3      0.5136      0.4241                    18                      0.0   \n",
              "4      0.6124      0.8108                    18                      0.0   \n",
              "\n",
              "   hospital_count_nearby  hospital_beds_nearby  school_count_nearby  \\\n",
              "0                      6                   0.0                   74   \n",
              "1                      6                   0.0                   74   \n",
              "2                      6                   0.0                   74   \n",
              "3                      6                   0.0                   78   \n",
              "4                      6                   0.0                   75   \n",
              "\n",
              "   school_capacity_nearby  \n",
              "0                     0.0  \n",
              "1                     0.0  \n",
              "2                     0.0  \n",
              "3                     0.0  \n",
              "4                     0.0  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import BallTree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SHELTER_CAP_COL = \"capacity\"\n",
        "HOSPITAL_BEDS_COL = \"beds\"\n",
        "SCHOOL_CAP_COL = \"capacity\"   # or \"students\"\n",
        "R_KM = 20.0\n",
        "\n",
        "def to_balltree(df: pd.DataFrame) -> BallTree:\n",
        "    coords = np.deg2rad(df[[\"lat\",\"lon\"]].values)\n",
        "    return BallTree(coords, metric=\"haversine\")\n",
        "\n",
        "def ensure_numeric_col(df: pd.DataFrame, col: str, default: float = 0.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Guarantee df[col] exists and is numeric.\n",
        "    If missing, create it with default.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    if col not in df.columns:\n",
        "        df[col] = default\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(default)\n",
        "    return df\n",
        "\n",
        "def aggregate_to_grid(grid_df, shelters_df, hospitals_df, schools_df, R_km=20.0):\n",
        "    g = grid_df.copy()\n",
        "\n",
        "    g[\"shelter_count_nearby\"] = 0\n",
        "    g[\"shelter_capacity_nearby\"] = 0.0\n",
        "    g[\"hospital_count_nearby\"] = 0\n",
        "    g[\"hospital_beds_nearby\"] = 0.0\n",
        "    g[\"school_count_nearby\"] = 0\n",
        "    g[\"school_capacity_nearby\"] = 0.0\n",
        "\n",
        "    g_coords = np.deg2rad(g[[\"lat\",\"lon\"]].values)\n",
        "    R = R_km / 6371.0  # km -> radians\n",
        "\n",
        "    # Shelters\n",
        "    if len(shelters_df) > 0:\n",
        "        s = ensure_numeric_col(shelters_df, SHELTER_CAP_COL, default=0.0)\n",
        "        tree = to_balltree(s)\n",
        "        ind = tree.query_radius(g_coords, r=R)\n",
        "        for i, nbrs in enumerate(ind):\n",
        "            g.at[i, \"shelter_count_nearby\"] = int(len(nbrs))\n",
        "            g.at[i, \"shelter_capacity_nearby\"] = float(s.iloc[nbrs][SHELTER_CAP_COL].sum())\n",
        "\n",
        "    # Hospitals\n",
        "    if len(hospitals_df) > 0:\n",
        "        h = ensure_numeric_col(hospitals_df, HOSPITAL_BEDS_COL, default=0.0)\n",
        "        tree = to_balltree(h)\n",
        "        ind = tree.query_radius(g_coords, r=R)\n",
        "        for i, nbrs in enumerate(ind):\n",
        "            g.at[i, \"hospital_count_nearby\"] = int(len(nbrs))\n",
        "            g.at[i, \"hospital_beds_nearby\"] = float(h.iloc[nbrs][HOSPITAL_BEDS_COL].sum())\n",
        "\n",
        "    # Schools\n",
        "    if len(schools_df) > 0:\n",
        "        c = schools_df.copy()\n",
        "        # choose capacity column if exists, else students, else 0\n",
        "        if SCHOOL_CAP_COL in c.columns:\n",
        "            c[\"__cap__\"] = pd.to_numeric(c[SCHOOL_CAP_COL], errors=\"coerce\").fillna(0.0)\n",
        "        elif \"students\" in c.columns:\n",
        "            c[\"__cap__\"] = pd.to_numeric(c[\"students\"], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            c[\"__cap__\"] = 0.0\n",
        "\n",
        "        tree = to_balltree(c)\n",
        "        ind = tree.query_radius(g_coords, r=R)\n",
        "        for i, nbrs in enumerate(ind):\n",
        "            g.at[i, \"school_count_nearby\"] = int(len(nbrs))\n",
        "            g.at[i, \"school_capacity_nearby\"] = float(c.iloc[nbrs][\"__cap__\"].sum())\n",
        "\n",
        "    return g\n",
        "\n",
        "grid_aug = aggregate_to_grid(grid, shelters, hospitals, schools, R_km=R_KM)\n",
        "print(\"✅ grid_aug:\", grid_aug.shape)\n",
        "grid_aug.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Build `nodes.csv` (grid nodes + static features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\recovery_dataset_out\\nodes.csv rows: 5122\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FIPS</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>RPL_THEMES</th>\n",
              "      <th>RPL_THEME1</th>\n",
              "      <th>RPL_THEME2</th>\n",
              "      <th>RPL_THEME3</th>\n",
              "      <th>RPL_THEME4</th>\n",
              "      <th>shelter_count_nearby</th>\n",
              "      <th>shelter_capacity_nearby</th>\n",
              "      <th>hospital_count_nearby</th>\n",
              "      <th>hospital_beds_nearby</th>\n",
              "      <th>school_count_nearby</th>\n",
              "      <th>school_capacity_nearby</th>\n",
              "      <th>node_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12001000201</td>\n",
              "      <td>29.654545</td>\n",
              "      <td>-82.333741</td>\n",
              "      <td>0.2704</td>\n",
              "      <td>0.5495</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.4077</td>\n",
              "      <td>0.6982</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>grid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12001000202</td>\n",
              "      <td>29.646142</td>\n",
              "      <td>-82.332971</td>\n",
              "      <td>0.3791</td>\n",
              "      <td>0.7495</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.4254</td>\n",
              "      <td>0.7609</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>grid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12001000301</td>\n",
              "      <td>29.666673</td>\n",
              "      <td>-82.331202</td>\n",
              "      <td>0.6544</td>\n",
              "      <td>0.5118</td>\n",
              "      <td>0.3832</td>\n",
              "      <td>0.5423</td>\n",
              "      <td>0.8684</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>grid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12001000302</td>\n",
              "      <td>29.683551</td>\n",
              "      <td>-82.330493</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.4880</td>\n",
              "      <td>0.3051</td>\n",
              "      <td>0.5136</td>\n",
              "      <td>0.4241</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>grid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12001000400</td>\n",
              "      <td>29.677770</td>\n",
              "      <td>-82.307900</td>\n",
              "      <td>0.7781</td>\n",
              "      <td>0.6217</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6124</td>\n",
              "      <td>0.8108</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>grid</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FIPS        lat        lon  RPL_THEMES  RPL_THEME1  RPL_THEME2  \\\n",
              "0  12001000201  29.654545 -82.333741      0.2704      0.5495      0.0024   \n",
              "1  12001000202  29.646142 -82.332971      0.3791      0.7495      0.0006   \n",
              "2  12001000301  29.666673 -82.331202      0.6544      0.5118      0.3832   \n",
              "3  12001000302  29.683551 -82.330493      0.4292      0.4880      0.3051   \n",
              "4  12001000400  29.677770 -82.307900      0.7781      0.6217      0.8306   \n",
              "\n",
              "   RPL_THEME3  RPL_THEME4  shelter_count_nearby  shelter_capacity_nearby  \\\n",
              "0      0.4077      0.6982                    17                      0.0   \n",
              "1      0.4254      0.7609                    17                      0.0   \n",
              "2      0.5423      0.8684                    17                      0.0   \n",
              "3      0.5136      0.4241                    18                      0.0   \n",
              "4      0.6124      0.8108                    18                      0.0   \n",
              "\n",
              "   hospital_count_nearby  hospital_beds_nearby  school_count_nearby  \\\n",
              "0                      6                   0.0                   74   \n",
              "1                      6                   0.0                   74   \n",
              "2                      6                   0.0                   74   \n",
              "3                      6                   0.0                   78   \n",
              "4                      6                   0.0                   75   \n",
              "\n",
              "   school_capacity_nearby node_type  \n",
              "0                     0.0      grid  \n",
              "1                     0.0      grid  \n",
              "2                     0.0      grid  \n",
              "3                     0.0      grid  \n",
              "4                     0.0      grid  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes = grid_aug.rename(columns={\"cell_id\": \"node_id\"}).copy()\n",
        "nodes[\"node_type\"] = \"grid\"\n",
        "\n",
        "# Optional: ensure SVI exists if you used a different name\n",
        "if \"SVI\" not in nodes.columns and \"vulnerability_score\" in nodes.columns:\n",
        "    nodes[\"SVI\"] = nodes[\"vulnerability_score\"]\n",
        "\n",
        "nodes_path = os.path.join(OUT_DIR, \"nodes.csv\")\n",
        "nodes.to_csv(nodes_path, index=False)\n",
        "print(\"Saved:\", nodes_path, \"rows:\", len(nodes))\n",
        "nodes.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "525edf68",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build `edges.csv` using KNN graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e947a8e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Required imports (must be run in the current kernel) =====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "4b356fad",
      "metadata": {},
      "outputs": [],
      "source": [
        "K_NEIGHBORS = 12\n",
        "\n",
        "def build_knn_edges(nodes_df, k=12):\n",
        "    coords = np.deg2rad(nodes_df[[\"lat\",\"lon\"]].values)\n",
        "\n",
        "    nn = NearestNeighbors(\n",
        "        n_neighbors=min(k + 1, len(nodes_df)),\n",
        "        metric=\"haversine\"\n",
        "    )\n",
        "    nn.fit(coords)\n",
        "\n",
        "    dist, ind = nn.kneighbors(coords)\n",
        "\n",
        "    src, dst, distance_km, weight = [], [], [], []\n",
        "\n",
        "    for i in range(len(nodes_df)):\n",
        "        for j_idx in range(1, ind.shape[1]):  # skip self-loop\n",
        "            j = int(ind[i, j_idx])\n",
        "            d_km = float(dist[i, j_idx]) * 6371.0  # radians → km\n",
        "\n",
        "            src.append(i)\n",
        "            dst.append(j)\n",
        "            distance_km.append(d_km)\n",
        "            weight.append(1.0 / (d_km + 1e-6))\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"src\": src,\n",
        "        \"dst\": dst,\n",
        "        \"distance_km\": distance_km,\n",
        "        \"weight\": weight\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2ef0b2a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: C:\\Users\\Adrija\\Downloads\\DFGCN\\recovery_dataset_out\\edges.csv\n",
            "Edges: 61464\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>dst</th>\n",
              "      <th>distance_km</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.937335</td>\n",
              "      <td>1.066854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.370738</td>\n",
              "      <td>0.729534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1.613922</td>\n",
              "      <td>0.619608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.615375</td>\n",
              "      <td>0.619051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1.635657</td>\n",
              "      <td>0.611375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   src  dst  distance_km    weight\n",
              "0    0    1     0.937335  1.066854\n",
              "1    0    2     1.370738  0.729534\n",
              "2    0   12     1.613922  0.619608\n",
              "3    0    5     1.615375  0.619051\n",
              "4    0    8     1.635657  0.611375"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes = grid_aug.copy()\n",
        "\n",
        "edges = build_knn_edges(nodes, k=K_NEIGHBORS)\n",
        "\n",
        "edges_path = os.path.join(OUT_DIR, \"edges.csv\")\n",
        "edges.to_csv(edges_path, index=False)\n",
        "\n",
        "print(\"Saved:\", edges_path)\n",
        "print(\"Edges:\", len(edges))\n",
        "edges.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "84643505",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['FIPS', 'lat', 'lon', 'RPL_THEMES', 'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'shelter_count_nearby', 'shelter_capacity_nearby', 'hospital_count_nearby', 'hospital_beds_nearby', 'school_count_nearby', 'school_capacity_nearby', 'node_type']\n"
          ]
        }
      ],
      "source": [
        "print(nodes.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Save DFGCN-style tensors: `X_static.npy`, `edge_index.npy`, `edge_weight.npy`, `node_ids.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "857c0d6e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ node_id source column used: FIPS\n",
            "X_static: (5122, 11)\n",
            "edge_index: (2, 61464) edge_weight: (61464,)\n",
            "Static feature columns sample: ['RPL_THEMES', 'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'shelter_count_nearby', 'shelter_capacity_nearby', 'hospital_count_nearby', 'hospital_beds_nearby', 'school_count_nearby', 'school_capacity_nearby']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1) Ensure we have a stable node_id\n",
        "nodes = nodes.copy()\n",
        "\n",
        "if \"node_id\" not in nodes.columns:\n",
        "    if \"cell_id\" in nodes.columns:\n",
        "        nodes[\"node_id\"] = nodes[\"cell_id\"].astype(str)\n",
        "    elif \"FIPS\" in nodes.columns:\n",
        "        nodes[\"node_id\"] = nodes[\"FIPS\"].astype(str)\n",
        "    else:\n",
        "        nodes[\"node_id\"] = [f\"node_{i}\" for i in range(len(nodes))]\n",
        "\n",
        "# (Optional) Make sure it's unique\n",
        "if nodes[\"node_id\"].duplicated().any():\n",
        "    # fall back to guaranteed-unique ids\n",
        "    nodes[\"node_id\"] = [f\"node_{i}\" for i in range(len(nodes))]\n",
        "\n",
        "# 2) Build node_to_idx mapping\n",
        "node_ids = nodes[\"node_id\"].astype(str).tolist()\n",
        "node_to_idx = {nid: i for i, nid in enumerate(node_ids)}\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"node_ids.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(node_to_idx, f, indent=2)\n",
        "\n",
        "# 3) Build X_static (exclude lat/lon, keep numeric features)\n",
        "numeric_cols = nodes.select_dtypes(include=[np.number]).columns.tolist()\n",
        "feat_cols = [c for c in numeric_cols if c not in [\"lat\", \"lon\"]]\n",
        "\n",
        "X_static = nodes[feat_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
        "np.save(os.path.join(OUT_DIR, \"X_static.npy\"), X_static)\n",
        "\n",
        "# 4) Edge arrays\n",
        "edge_index = np.vstack([\n",
        "    edges[\"src\"].to_numpy(np.int64),\n",
        "    edges[\"dst\"].to_numpy(np.int64)\n",
        "])\n",
        "edge_weight = edges[\"weight\"].to_numpy(np.float32)\n",
        "\n",
        "np.save(os.path.join(OUT_DIR, \"edge_index.npy\"), edge_index)\n",
        "np.save(os.path.join(OUT_DIR, \"edge_weight.npy\"), edge_weight)\n",
        "\n",
        "print(\"✅ node_id source column used:\",\n",
        "      \"cell_id\" if \"cell_id\" in nodes.columns else (\"FIPS\" if \"FIPS\" in nodes.columns else \"generated\"))\n",
        "print(\"X_static:\", X_static.shape)\n",
        "print(\"edge_index:\", edge_index.shape, \"edge_weight:\", edge_weight.shape)\n",
        "print(\"Static feature columns sample:\", feat_cols[:12])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c56261d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['FIPS', 'lat', 'lon', 'RPL_THEMES', 'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'shelter_count_nearby', 'shelter_capacity_nearby', 'hospital_count_nearby', 'hospital_beds_nearby', 'school_count_nearby', 'school_capacity_nearby', 'node_id']\n"
          ]
        }
      ],
      "source": [
        "print(nodes.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Optional: Build `X_dynamic.npy` from `weather_ensemble.csv` (T, N, F, E)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "3cff0673",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['t', 'node_id', 'ens', 'wind10m', 'precip', 'mslp']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Unique node_id sample: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
            " '16' '17' '18' '19']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "w = pd.read_csv(WEATHER_ENSEMBLE_CSV)\n",
        "print(w.columns.tolist())\n",
        "print(w[\"node_id\"].head(20).tolist())\n",
        "print(\"Unique node_id sample:\", w[\"node_id\"].astype(str).dropna().unique()[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "55fa3182",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Rewritten weather_ensemble.csv with tract-FIPS node_id\n",
            "0    12001000201\n",
            "1    12001000201\n",
            "2    12001000201\n",
            "3    12001000201\n",
            "4    12001000201\n",
            "Name: node_id, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "w = pd.read_csv(WEATHER_ENSEMBLE_CSV)\n",
        "\n",
        "# Make sure your nodes are in the SAME ORDER used when weather was generated\n",
        "# If weather was generated using the same 'nodes' dataframe rows, this works directly.\n",
        "idx_to_nodeid = dict(enumerate(nodes[\"node_id\"].astype(str).tolist()))\n",
        "\n",
        "w[\"node_id\"] = w[\"node_id\"].astype(int).map(idx_to_nodeid)\n",
        "w = w.dropna(subset=[\"node_id\"])\n",
        "\n",
        "w.to_csv(WEATHER_ENSEMBLE_CSV, index=False)\n",
        "print(\"✅ Rewritten weather_ensemble.csv with tract-FIPS node_id\")\n",
        "print(w[\"node_id\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "1e0af16b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def build_X_dynamic(\n",
        "    weather_csv: str,\n",
        "    node_to_idx: dict,\n",
        "    dyn_cols: list\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dynamic tensor:\n",
        "    X_dynamic shape = (T, N, F, E)\n",
        "\n",
        "    T = number of timesteps\n",
        "    N = number of nodes\n",
        "    F = number of dynamic features\n",
        "    E = number of ensemble members\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load & basic cleaning\n",
        "    # -----------------------------\n",
        "    w = pd.read_csv(weather_csv)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required = {\"t\", \"ens\", \"node_id\"}\n",
        "    missing = required - set(w.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"weather_ensemble.csv missing columns: {missing}\")\n",
        "\n",
        "    # Standardize types\n",
        "    w[\"node_id\"] = w[\"node_id\"].astype(str)\n",
        "    w[\"t\"] = pd.to_numeric(w[\"t\"], errors=\"coerce\")\n",
        "    w[\"ens\"] = pd.to_numeric(w[\"ens\"], errors=\"coerce\")\n",
        "\n",
        "    # Drop invalid rows\n",
        "    w = w.dropna(subset=[\"t\", \"ens\", \"node_id\"])\n",
        "\n",
        "    # -----------------------------\n",
        "    # Filter to nodes in graph\n",
        "    # -----------------------------\n",
        "    before = len(w)\n",
        "    w = w[w[\"node_id\"].isin(node_to_idx)]\n",
        "    after = len(w)\n",
        "\n",
        "    if after == 0:\n",
        "        raise ValueError(\n",
        "            \"❌ No matching node IDs between weather_ensemble.csv and graph.\\n\"\n",
        "            f\"Example graph node_ids: {list(node_to_idx.keys())[:10]}\\n\"\n",
        "            f\"Example weather node_ids: {w['node_id'].unique()[:10]}\\n\"\n",
        "            \"→ Fix by aligning weather node_id with graph node_id (FIPS).\"\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Weather rows kept after node-id filter: {after}/{before}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Dimensions\n",
        "    # -----------------------------\n",
        "    T = int(w[\"t\"].max()) + 1\n",
        "    E = int(w[\"ens\"].max()) + 1\n",
        "    N = len(node_to_idx)\n",
        "    F = len(dyn_cols)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Allocate tensor\n",
        "    # -----------------------------\n",
        "    Xd = np.zeros((T, N, F, E), dtype=np.float32)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Fill tensor\n",
        "    # -----------------------------\n",
        "    for _, r in w.iterrows():\n",
        "        t = int(r[\"t\"])\n",
        "        e = int(r[\"ens\"])\n",
        "        n = node_to_idx[r[\"node_id\"]]\n",
        "\n",
        "        Xd[t, n, :, e] = np.array(\n",
        "            [float(r.get(c, 0.0)) for c in dyn_cols],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    return Xd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "80765b18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Weather rows kept after node-id filter: 48000/48000\n",
            "✅ Saved X_dynamic.npy\n",
            "Shape (T, N, F, E): (24, 5122, 4, 10)\n"
          ]
        }
      ],
      "source": [
        "DYN_COLS = [\"wind10m\", \"precip\", \"mslp\", \"gust\"]\n",
        "\n",
        "X_dynamic = build_X_dynamic(\n",
        "    WEATHER_ENSEMBLE_CSV,\n",
        "    node_to_idx,\n",
        "    DYN_COLS\n",
        ")\n",
        "\n",
        "np.save(os.path.join(OUT_DIR, \"X_dynamic.npy\"), X_dynamic)\n",
        "\n",
        "print(\"✅ Saved X_dynamic.npy\")\n",
        "print(\"Shape (T, N, F, E):\", X_dynamic.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Optional: Build `Y.npy` from `recovery_labels.csv` (T, N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "1829004a",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shelter file missing lat/lon columns. Found: ['OBJECTID_1', 'OBJECTID', 'Name', 'Address', 'City', 'Zip', 'COUNTY', 'X', 'Y', 'LONGLAT', 'USNG', 'FACILITY_T', 'SHELTER_TY', 'RPC_Region', 'Year_Built', 'Building', 'AS_IS_A_Re', 'Generator_', 'PreMitigat', 'PreMitig_1', 'EHPA_Capac', 'EHPA_Squar', 'Retrofit_C', 'Retrofit_S', 'Risk_Capac', 'Risk_Squar', 'Planned_Us', 'FundingSou', 'Evacuation', 'SURGE_ZONE', 'FLOOD_ZONE', 'EHPA', 'General_Po', 'SPECIAL_NE', 'Pet_Friend', 'Does_not_m', 'Survey_Nee', 'Asset_Elevation', 'Owned_Maintained', 'Asset', 'Asset_Sub_Type', 'Asset_Type', 'Asset_Group', 'Asset_Relevancy', 'Asset_ID', 'Municipality', 'Water_Management_District', 'Souce_Dataset_Entity', 'Souce_Dataset_Name', 'Source_Asset_ID', 'Restricted_Public_Disclosure', 'Notes', 'Regional_Planning_Council', 'FDEP_Regulatory_District', 'RCP_Region', 'FDOT_District', 'House_District_Number', 'Senate_District_Number', 'CTF_RL_2020', 'CTF_RL_2040_IL', 'CTF_RL_2040_INT', 'CTF_RL_2070_IL', 'CTF_RL_2070_INT', 'SSF_RL_100YR_CURR', 'SSF_RL_100YR_2040_IL', 'SSF_RL_100YR_2040_INT', 'SSF_RL_100YR_2070_IL', 'SSF_RL_100YR_2070_INT', 'SSF_RL_500YR_CURR', 'SSF_RL_500YR_2040_IL', 'SSF_RL_500YR_2040_INT', 'SSF_RL_500YR_2070_IL', 'SSF_RL_500YR_2070_INT', 'RIF_RL_100YR_2020', 'RIF_RL_100YR_2040', 'RIF_RL_100YR_2070', 'RIF_RL_500YR_2020', 'RIF_RL_500YR_2040', 'RIF_RL_500YR_2070', 'facility_type', 'facility_id']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[59], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# ---------- RUN ----------\u001b[39;00m\n\u001b[0;32m    123\u001b[0m NODE_LABELS_CSV \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecovery_labels_node.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m ynode \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_node_labels_from_facility_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRECOVERY_LABELS_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshelters_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSHELTERS_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhospitals_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHOSPITALS_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschools_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCHOOLS_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_to_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_to_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_node_labels_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNODE_LABELS_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43magg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    134\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m Y \u001b[38;5;241m=\u001b[39m build_Y_from_node_labels(ynode, node_to_idx, y_col\u001b[38;5;241m=\u001b[39mY_COL)\n\u001b[0;32m    137\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), Y)\n",
            "Cell \u001b[1;32mIn[59], line 51\u001b[0m, in \u001b[0;36mbuild_node_labels_from_facility_labels\u001b[1;34m(labels_csv, shelters_csv, hospitals_csv, schools_csv, nodes_df, node_to_idx, out_node_labels_csv, agg)\u001b[0m\n\u001b[0;32m     47\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacility_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacility_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m---> 51\u001b[0m shelters \u001b[38;5;241m=\u001b[39m \u001b[43mload_fac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshelters_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshelter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m hospitals \u001b[38;5;241m=\u001b[39m load_fac(hospitals_csv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhospital\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m schools \u001b[38;5;241m=\u001b[39m load_fac(schools_csv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[59], line 43\u001b[0m, in \u001b[0;36mbuild_node_labels_from_facility_labels.<locals>.load_fac\u001b[1;34m(path, kind)\u001b[0m\n\u001b[0;32m     41\u001b[0m lon_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lat_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m lon_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file missing lat/lon columns. Found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[lat_col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[lon_col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: shelter file missing lat/lon columns. Found: ['OBJECTID_1', 'OBJECTID', 'Name', 'Address', 'City', 'Zip', 'COUNTY', 'X', 'Y', 'LONGLAT', 'USNG', 'FACILITY_T', 'SHELTER_TY', 'RPC_Region', 'Year_Built', 'Building', 'AS_IS_A_Re', 'Generator_', 'PreMitigat', 'PreMitig_1', 'EHPA_Capac', 'EHPA_Squar', 'Retrofit_C', 'Retrofit_S', 'Risk_Capac', 'Risk_Squar', 'Planned_Us', 'FundingSou', 'Evacuation', 'SURGE_ZONE', 'FLOOD_ZONE', 'EHPA', 'General_Po', 'SPECIAL_NE', 'Pet_Friend', 'Does_not_m', 'Survey_Nee', 'Asset_Elevation', 'Owned_Maintained', 'Asset', 'Asset_Sub_Type', 'Asset_Type', 'Asset_Group', 'Asset_Relevancy', 'Asset_ID', 'Municipality', 'Water_Management_District', 'Souce_Dataset_Entity', 'Souce_Dataset_Name', 'Source_Asset_ID', 'Restricted_Public_Disclosure', 'Notes', 'Regional_Planning_Council', 'FDEP_Regulatory_District', 'RCP_Region', 'FDOT_District', 'House_District_Number', 'Senate_District_Number', 'CTF_RL_2020', 'CTF_RL_2040_IL', 'CTF_RL_2040_INT', 'CTF_RL_2070_IL', 'CTF_RL_2070_INT', 'SSF_RL_100YR_CURR', 'SSF_RL_100YR_2040_IL', 'SSF_RL_100YR_2040_INT', 'SSF_RL_100YR_2070_IL', 'SSF_RL_100YR_2070_INT', 'SSF_RL_500YR_CURR', 'SSF_RL_500YR_2040_IL', 'SSF_RL_500YR_2040_INT', 'SSF_RL_500YR_2070_IL', 'SSF_RL_500YR_2070_INT', 'RIF_RL_100YR_2020', 'RIF_RL_100YR_2040', 'RIF_RL_100YR_2070', 'RIF_RL_500YR_2020', 'RIF_RL_500YR_2040', 'RIF_RL_500YR_2070', 'facility_type', 'facility_id']"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "Y_COL = \"recovery_index\"\n",
        "\n",
        "def build_node_labels_from_facility_labels(\n",
        "    labels_csv,\n",
        "    shelters_csv,\n",
        "    hospitals_csv,\n",
        "    schools_csv,\n",
        "    nodes_df,\n",
        "    node_to_idx,\n",
        "    out_node_labels_csv,\n",
        "    agg=\"mean\"   # mean or max\n",
        "):\n",
        "    # --- load facility-level labels ---\n",
        "    y = pd.read_csv(labels_csv)\n",
        "    y[\"t\"] = pd.to_numeric(y[\"t\"], errors=\"coerce\")\n",
        "    y[Y_COL] = pd.to_numeric(y[Y_COL], errors=\"coerce\")\n",
        "    y = y.dropna(subset=[\"t\", Y_COL, \"facility_id\", \"facility_type\"]).copy()\n",
        "    y[\"facility_id\"] = y[\"facility_id\"].astype(str)\n",
        "    y[\"facility_type\"] = y[\"facility_type\"].astype(str).str.lower()\n",
        "\n",
        "    # --- load facilities with lat/lon ---\n",
        "    def load_fac(path, kind):\n",
        "        df = pd.read_csv(path)\n",
        "        df = df.copy()\n",
        "        df[\"facility_type\"] = kind\n",
        "        # detect id col\n",
        "        if \"facility_id\" not in df.columns:\n",
        "            if \"id\" in df.columns:\n",
        "                df[\"facility_id\"] = df[\"id\"].astype(str)\n",
        "            else:\n",
        "                df[\"facility_id\"] = [f\"{kind}_{i}\" for i in range(len(df))]\n",
        "        df[\"facility_id\"] = df[\"facility_id\"].astype(str)\n",
        "\n",
        "        # detect lat/lon\n",
        "        lat_col = \"lat\" if \"lat\" in df.columns else (\"latitude\" if \"latitude\" in df.columns else None)\n",
        "        lon_col = \"lon\" if \"lon\" in df.columns else (\"longitude\" if \"longitude\" in df.columns else None)\n",
        "        if lat_col is None or lon_col is None:\n",
        "            raise ValueError(f\"{kind} file missing lat/lon columns. Found: {df.columns.tolist()}\")\n",
        "\n",
        "        df[\"lat\"] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
        "        df[\"lon\"] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"lat\", \"lon\"]).copy()\n",
        "\n",
        "        return df[[\"facility_id\",\"facility_type\",\"lat\",\"lon\"]]\n",
        "\n",
        "    shelters = load_fac(shelters_csv, \"shelter\")\n",
        "    hospitals = load_fac(hospitals_csv, \"hospital\")\n",
        "    schools = load_fac(schools_csv, \"school\")\n",
        "\n",
        "    fac = pd.concat([shelters, hospitals, schools], ignore_index=True)\n",
        "\n",
        "    # --- merge labels with facility coords ---\n",
        "    y = y.merge(fac, on=[\"facility_id\",\"facility_type\"], how=\"inner\")\n",
        "    if len(y) == 0:\n",
        "        raise ValueError(\n",
        "            \"After joining labels with facility coordinates, no rows remain.\\n\"\n",
        "            \"Check that facility_id + facility_type match between recovery_labels.csv and facilities CSVs.\"\n",
        "        )\n",
        "\n",
        "    # --- map facilities -> nearest node ---\n",
        "    nodes_df = nodes_df.copy()\n",
        "    if \"node_id\" not in nodes_df.columns:\n",
        "        # if you used cell_id or FIPS, make sure node_id exists\n",
        "        if \"cell_id\" in nodes_df.columns:\n",
        "            nodes_df[\"node_id\"] = nodes_df[\"cell_id\"].astype(str)\n",
        "        elif \"FIPS\" in nodes_df.columns:\n",
        "            nodes_df[\"node_id\"] = nodes_df[\"FIPS\"].astype(str)\n",
        "        else:\n",
        "            nodes_df[\"node_id\"] = [f\"node_{i}\" for i in range(len(nodes_df))]\n",
        "\n",
        "    nodes_df[\"node_id\"] = nodes_df[\"node_id\"].astype(str)\n",
        "\n",
        "    # Build BallTree for nodes (centroids)\n",
        "    coords_nodes = np.deg2rad(nodes_df[[\"lat\",\"lon\"]].values)\n",
        "    tree = BallTree(coords_nodes, metric=\"haversine\")\n",
        "\n",
        "    coords_fac = np.deg2rad(y[[\"lat\",\"lon\"]].values)\n",
        "    dist, ind = tree.query(coords_fac, k=1)\n",
        "    y[\"node_id\"] = nodes_df.iloc[ind.flatten()][\"node_id\"].astype(str).values\n",
        "\n",
        "    # --- aggregate facility recovery per (t, node_id) ---\n",
        "    if agg == \"max\":\n",
        "        ynode = y.groupby([\"t\",\"node_id\"], as_index=False)[Y_COL].max()\n",
        "    else:\n",
        "        ynode = y.groupby([\"t\",\"node_id\"], as_index=False)[Y_COL].mean()\n",
        "\n",
        "    # filter nodes existing in graph mapping\n",
        "    ynode = ynode[ynode[\"node_id\"].isin(node_to_idx)].copy()\n",
        "    if len(ynode) == 0:\n",
        "        raise ValueError(\"No node-level labels match node_to_idx after aggregation.\")\n",
        "\n",
        "    ynode.to_csv(out_node_labels_csv, index=False)\n",
        "    print(\"✅ Saved node-level labels:\", out_node_labels_csv, \"rows:\", len(ynode))\n",
        "\n",
        "    return ynode\n",
        "\n",
        "def build_Y_from_node_labels(node_labels_df, node_to_idx, y_col=\"recovery_index\"):\n",
        "    # ensure numeric\n",
        "    node_labels_df[\"t\"] = pd.to_numeric(node_labels_df[\"t\"], errors=\"coerce\")\n",
        "    node_labels_df[y_col] = pd.to_numeric(node_labels_df[y_col], errors=\"coerce\")\n",
        "    node_labels_df = node_labels_df.dropna(subset=[\"t\", \"node_id\", y_col]).copy()\n",
        "    node_labels_df[\"node_id\"] = node_labels_df[\"node_id\"].astype(str)\n",
        "\n",
        "    T = int(node_labels_df[\"t\"].max()) + 1\n",
        "    N = len(node_to_idx)\n",
        "    Y = np.full((T, N), np.nan, dtype=np.float32)\n",
        "\n",
        "    for r in node_labels_df.itertuples(index=False):\n",
        "        t = int(getattr(r, \"t\"))\n",
        "        nid = str(getattr(r, \"node_id\"))\n",
        "        if nid in node_to_idx:\n",
        "            Y[t, node_to_idx[nid]] = float(getattr(r, y_col))\n",
        "\n",
        "    Y = pd.DataFrame(Y).ffill().fillna(0.0).to_numpy(dtype=np.float32)\n",
        "    return Y\n",
        "\n",
        "# ---------- RUN ----------\n",
        "NODE_LABELS_CSV = os.path.join(OUT_DIR, \"recovery_labels_node.csv\")\n",
        "\n",
        "ynode = build_node_labels_from_facility_labels(\n",
        "    labels_csv=RECOVERY_LABELS_CSV,\n",
        "    shelters_csv=SHELTERS_CSV,\n",
        "    hospitals_csv=HOSPITALS_CSV,\n",
        "    schools_csv=SCHOOLS_CSV,\n",
        "    nodes_df=nodes,\n",
        "    node_to_idx=node_to_idx,\n",
        "    out_node_labels_csv=NODE_LABELS_CSV,\n",
        "    agg=\"mean\"\n",
        ")\n",
        "\n",
        "Y = build_Y_from_node_labels(ynode, node_to_idx, y_col=Y_COL)\n",
        "np.save(os.path.join(OUT_DIR, \"Y.npy\"), Y)\n",
        "\n",
        "print(\"✅ Saved Y.npy:\", Y.shape)\n",
        "print(ynode.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "5d5d5102",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_fac(path, kind):\n",
        "    df = pd.read_csv(path).copy()\n",
        "    df[\"facility_type\"] = kind\n",
        "\n",
        "    # --- ensure facility_id exists ---\n",
        "    if \"facility_id\" not in df.columns:\n",
        "        if \"id\" in df.columns:\n",
        "            df[\"facility_id\"] = df[\"id\"].astype(str)\n",
        "        elif \"Asset_ID\" in df.columns:\n",
        "            df[\"facility_id\"] = df[\"Asset_ID\"].astype(str)\n",
        "        else:\n",
        "            df[\"facility_id\"] = [f\"{kind}_{i}\" for i in range(len(df))]\n",
        "    df[\"facility_id\"] = df[\"facility_id\"].astype(str)\n",
        "\n",
        "    # --- detect coordinate formats ---\n",
        "    # Case 1: already lat/lon\n",
        "    if \"lat\" in df.columns and \"lon\" in df.columns:\n",
        "        df[\"lat\"] = pd.to_numeric(df[\"lat\"], errors=\"coerce\")\n",
        "        df[\"lon\"] = pd.to_numeric(df[\"lon\"], errors=\"coerce\")\n",
        "\n",
        "    # Case 2: latitude/longitude\n",
        "    elif \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
        "        df[\"lat\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
        "        df[\"lon\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
        "\n",
        "    # Case 3: X/Y (most GIS exports: X=lon, Y=lat)\n",
        "    elif \"X\" in df.columns and \"Y\" in df.columns:\n",
        "        df[\"lon\"] = pd.to_numeric(df[\"X\"], errors=\"coerce\")\n",
        "        df[\"lat\"] = pd.to_numeric(df[\"Y\"], errors=\"coerce\")\n",
        "\n",
        "    # Case 4: LONGLAT string\n",
        "    elif \"LONGLAT\" in df.columns:\n",
        "        def parse_lonlat(v):\n",
        "            if pd.isna(v):\n",
        "                return (np.nan, np.nan)\n",
        "            s = str(v).strip()\n",
        "            # remove parentheses\n",
        "            s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            # split by comma or space\n",
        "            parts = re.split(r\"[,\\s]+\", s)\n",
        "            parts = [p for p in parts if p != \"\"]\n",
        "            if len(parts) < 2:\n",
        "                return (np.nan, np.nan)\n",
        "            lon = pd.to_numeric(parts[0], errors=\"coerce\")\n",
        "            lat = pd.to_numeric(parts[1], errors=\"coerce\")\n",
        "            return (lat, lon)\n",
        "\n",
        "        latlon = df[\"LONGLAT\"].apply(parse_lonlat)\n",
        "        df[\"lat\"] = latlon.apply(lambda x: x[0])\n",
        "        df[\"lon\"] = latlon.apply(lambda x: x[1])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"{kind} file missing coordinates.\\n\"\n",
        "            f\"Expected lat/lon OR latitude/longitude OR X/Y OR LONGLAT.\\n\"\n",
        "            f\"Found columns: {df.columns.tolist()}\"\n",
        "        )\n",
        "\n",
        "    # drop invalid coords\n",
        "    df = df.dropna(subset=[\"lat\", \"lon\"]).copy()\n",
        "\n",
        "    # sanity filter (Florida bounds-ish, optional)\n",
        "    df = df[(df[\"lat\"].between(24, 32)) & (df[\"lon\"].between(-88, -79))].copy()\n",
        "\n",
        "    return df[[\"facility_id\", \"facility_type\", \"lat\", \"lon\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'node_id'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Adrija\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'node_id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[62], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(RECOVERY_LABELS_CSV):\n\u001b[1;32m---> 21\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_Y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRECOVERY_LABELS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_COL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m), Y)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved Y.npy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape)\n",
            "Cell \u001b[1;32mIn[62], line 5\u001b[0m, in \u001b[0;36mbuild_Y\u001b[1;34m(labels_csv, node_to_idx, y_col)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_Y\u001b[39m(labels_csv, node_to_idx, y_col):\n\u001b[0;32m      4\u001b[0m     ydf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(labels_csv)\n\u001b[1;32m----> 5\u001b[0m     ydf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mydf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ydf \u001b[38;5;241m=\u001b[39m ydf[ydf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(node_to_idx)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      8\u001b[0m     T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(ydf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Adrija\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\Adrija\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'node_id'"
          ]
        }
      ],
      "source": [
        "Y_COL = \"recovery_index\"  # edit if your column name differs\n",
        "\n",
        "def build_Y(labels_csv, node_to_idx, y_col):\n",
        "    ydf = pd.read_csv(labels_csv)\n",
        "    ydf[\"node_id\"] = ydf[\"node_id\"].astype(str)\n",
        "    ydf = ydf[ydf[\"node_id\"].isin(node_to_idx)].copy()\n",
        "\n",
        "    T = int(ydf[\"t\"].max()) + 1\n",
        "    N = len(node_to_idx)\n",
        "    Y = np.full((T, N), np.nan, dtype=np.float32)\n",
        "\n",
        "    for _, r in ydf.iterrows():\n",
        "        t = int(r[\"t\"])\n",
        "        n = node_to_idx[r[\"node_id\"]]\n",
        "        Y[t, n] = float(r[y_col])\n",
        "\n",
        "    Y = pd.DataFrame(Y).fillna(method=\"ffill\").fillna(0.0).to_numpy(dtype=np.float32)\n",
        "    return Y\n",
        "\n",
        "if os.path.exists(RECOVERY_LABELS_CSV):\n",
        "    Y = build_Y(RECOVERY_LABELS_CSV, node_to_idx, Y_COL)\n",
        "    np.save(os.path.join(OUT_DIR, \"Y.npy\"), Y)\n",
        "    print(\"Saved Y.npy:\", Y.shape)\n",
        "else:\n",
        "    print(\"recovery_labels.csv not found. Upload it and rerun this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Verify outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in OUT_DIR:\n",
            " - X_dynamic.npy\n",
            " - X_static.npy\n",
            " - edge_index.npy\n",
            " - edge_weight.npy\n",
            " - edges.csv\n",
            " - node_ids.json\n",
            " - nodes.csv\n",
            "\n",
            "Shapes:\n",
            "X_static: (5122, 11)\n",
            "edge_index: (2, 61464)\n",
            "edge_weight: (61464,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Files in OUT_DIR:\")\n",
        "for fn in sorted(os.listdir(OUT_DIR)):\n",
        "    print(\" -\", fn)\n",
        "\n",
        "X_static_chk = np.load(os.path.join(OUT_DIR, \"X_static.npy\"))\n",
        "edge_index_chk = np.load(os.path.join(OUT_DIR, \"edge_index.npy\"))\n",
        "edge_weight_chk = np.load(os.path.join(OUT_DIR, \"edge_weight.npy\"))\n",
        "\n",
        "print(\"\\nShapes:\")\n",
        "print(\"X_static:\", X_static_chk.shape)\n",
        "print(\"edge_index:\", edge_index_chk.shape)\n",
        "print(\"edge_weight:\", edge_weight_chk.shape)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
