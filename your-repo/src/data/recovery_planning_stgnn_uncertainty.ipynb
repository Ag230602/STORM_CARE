{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recovery/Planning Notebook\n",
        "**ST-GNN + Uncertainty from Weather Ensembles (GenCast / FourCastNet3 style)**\n",
        "\n",
        "Template for:\n",
        "- Load facility + vulnerability CSVs (Shelters, Hospitals, Schools, Vulnerability grid)\n",
        "- Build a distance-based graph\n",
        "- Load weather ensemble time series (GenCast/FourCastNet3 outputs interpolated to your nodes)\n",
        "- Train an ST-GNN model with quantile outputs (P10/P50/P90)\n",
        "- Predict recovery distributions and run a simple planning allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Install dependencies (run once)\n",
        "!pip -q install pandas numpy scikit-learn torch torch-geometric geopy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "from torch_geometric.nn import GCNConv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) File paths (edit these)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHELTERS_CSV = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\shelters.csv\"\n",
        "HOSPITALS_CSV = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\hospitals.csv\"\n",
        "SCHOOLS_CSV   = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\facilities\\\\schools.csv\"\n",
        "VULN_GRID_CSV = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\vulnerability\\\\\\\\vulnerability_grid_clean.csv\"\n",
        "\n",
        "# Optional (for training)\n",
        "WEATHER_ENSEMBLE_CSV = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\data\\\\raw\\\\weather_ensemble.csv\"  # t, node_id, ens, wind10m, precip, mslp, gust, ...\n",
        "RECOVERY_LABELS_CSV  = \"C:\\\\Users\\\\Adrija\\\\Downloads\\\\DFGCN\\\\data\\\\raw\\\\recovery_labels.csv\"   # t, node_id, recovery_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load CSVs and build node table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "All inputs must contain 'lat' and 'lon' columns.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n\u001b[1;32m---> 30\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mload_facilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHELTERS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHOSPITALS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCHOOLS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVULN_GRID_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nodes\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     32\u001b[0m nodes\u001b[38;5;241m.\u001b[39mhead()\n",
            "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mload_facilities\u001b[1;34m(shelters_csv, hospitals_csv, schools_csv, vuln_grid_csv)\u001b[0m\n\u001b[0;32m     19\u001b[0m nodes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([grid, shelters, hospitals, schools], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m nodes\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m nodes\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll inputs must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m nodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(nodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m nodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(nodes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: All inputs must contain 'lat' and 'lon' columns."
          ]
        }
      ],
      "source": [
        "def load_facilities(shelters_csv, hospitals_csv, schools_csv, vuln_grid_csv):\n",
        "    shelters = pd.read_csv(shelters_csv)\n",
        "    hospitals = pd.read_csv(hospitals_csv)\n",
        "    schools = pd.read_csv(schools_csv)\n",
        "    grid = pd.read_csv(vuln_grid_csv)\n",
        "\n",
        "    # Standardize facilities\n",
        "    for df, kind in [(shelters, \"shelter\"), (hospitals, \"hospital\"), (schools, \"school\")]:\n",
        "        df[\"kind\"] = kind\n",
        "        if \"id\" not in df.columns:\n",
        "            df[\"id\"] = [f\"{kind}_{i}\" for i in range(len(df))]\n",
        "\n",
        "    # Standardize grid\n",
        "    if \"cell_id\" not in grid.columns:\n",
        "        grid[\"cell_id\"] = [f\"cell_{i}\" for i in range(len(grid))]\n",
        "    grid[\"kind\"] = \"grid\"\n",
        "    grid = grid.rename(columns={\"cell_id\": \"id\"})\n",
        "\n",
        "    nodes = pd.concat([grid, shelters, hospitals, schools], ignore_index=True)\n",
        "\n",
        "    if \"lat\" not in nodes.columns or \"lon\" not in nodes.columns:\n",
        "        raise ValueError(\"All inputs must contain 'lat' and 'lon' columns.\")\n",
        "\n",
        "    nodes[\"lat\"] = pd.to_numeric(nodes[\"lat\"], errors=\"coerce\")\n",
        "    nodes[\"lon\"] = pd.to_numeric(nodes[\"lon\"], errors=\"coerce\")\n",
        "    nodes = nodes.dropna(subset=[\"lat\",\"lon\"]).reset_index(drop=True)\n",
        "\n",
        "    return nodes\n",
        "\n",
        "nodes = load_facilities(SHELTERS_CSV, HOSPITALS_CSV, SCHOOLS_CSV, VULN_GRID_CSV)\n",
        "print(\"Nodes:\", nodes.shape)\n",
        "nodes.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Build graph edges (distance-based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def haversine_graph(nodes_df, R_km=15.0, max_neighbors=20):\n",
        "    coords = np.deg2rad(nodes_df[[\"lat\",\"lon\"]].values)\n",
        "    tree = BallTree(coords, metric=\"haversine\")\n",
        "    R = R_km / 6371.0\n",
        "\n",
        "    ind, dist = tree.query_radius(coords, r=R, return_distance=True, sort_results=True)\n",
        "\n",
        "    edge_src, edge_dst, edge_w = [], [], []\n",
        "    for i, (nbrs, dists) in enumerate(zip(ind, dist)):\n",
        "        nbrs = nbrs[:max_neighbors]\n",
        "        dists = dists[:max_neighbors]\n",
        "        for j, d in zip(nbrs, dists):\n",
        "            if i == j:\n",
        "                continue\n",
        "            edge_src.append(i)\n",
        "            edge_dst.append(j)\n",
        "            edge_w.append(1.0 / (d + 1e-6))\n",
        "\n",
        "    edge_index = np.vstack([edge_src, edge_dst]).astype(np.int64)\n",
        "    edge_weight = np.array(edge_w, dtype=np.float32)\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "edge_index, edge_weight = haversine_graph(nodes, R_km=15, max_neighbors=20)\n",
        "print(\"edge_index:\", edge_index.shape, \"edge_weight:\", edge_weight.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Static features (per node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_static_features(nodes_df):\n",
        "    numeric_cols = nodes_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    feat_cols = [c for c in numeric_cols if c not in [\"lat\",\"lon\"]]\n",
        "    Xs = nodes_df[feat_cols].fillna(0.0).to_numpy(dtype=np.float32)\n",
        "    return Xs, feat_cols\n",
        "\n",
        "X_static, static_cols = build_static_features(nodes)\n",
        "print(\"X_static:\", X_static.shape)\n",
        "print(\"Example static columns:\", static_cols[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Load weather ensembles to tensor (T, N, F, E)\n",
        "\n",
        "CSV must contain: `t`, `node_id`, `ens` and dynamic columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_weather_ensembles(weather_csv, nodes_df, dyn_cols=(\"wind10m\",\"precip\",\"mslp\",\"gust\")):\n",
        "    w = pd.read_csv(weather_csv)\n",
        "    node_to_idx = {nid:i for i, nid in enumerate(nodes_df[\"id\"].tolist())}\n",
        "\n",
        "    T = int(w[\"t\"].max()) + 1\n",
        "    E = int(w[\"ens\"].max()) + 1\n",
        "    N = len(nodes_df)\n",
        "    F = len(dyn_cols)\n",
        "\n",
        "    Xd = np.zeros((T, N, F, E), dtype=np.float32)\n",
        "\n",
        "    for _, row in w.iterrows():\n",
        "        t = int(row[\"t\"]); e = int(row[\"ens\"])\n",
        "        nid = row[\"node_id\"]\n",
        "        if nid not in node_to_idx:\n",
        "            continue\n",
        "        n = node_to_idx[nid]\n",
        "        Xd[t, n, :, e] = np.array([row[c] for c in dyn_cols], dtype=np.float32)\n",
        "\n",
        "    return Xd\n",
        "\n",
        "# Uncomment when you have the file:\n",
        "# X_dynamic = load_weather_ensembles(WEATHER_ENSEMBLE_CSV, nodes)\n",
        "# print(\"X_dynamic:\", X_dynamic.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Load recovery labels (T, N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_recovery_labels(labels_csv, nodes_df, y_col=\"recovery_index\"):\n",
        "    ydf = pd.read_csv(labels_csv)\n",
        "    node_to_idx = {nid:i for i, nid in enumerate(nodes_df[\"id\"].tolist())}\n",
        "\n",
        "    T = int(ydf[\"t\"].max()) + 1\n",
        "    N = len(nodes_df)\n",
        "    Y = np.full((T, N), np.nan, dtype=np.float32)\n",
        "\n",
        "    for _, row in ydf.iterrows():\n",
        "        t = int(row[\"t\"])\n",
        "        nid = row[\"node_id\"]\n",
        "        if nid not in node_to_idx:\n",
        "            continue\n",
        "        n = node_to_idx[nid]\n",
        "        Y[t, n] = float(row[y_col])\n",
        "\n",
        "    Y = pd.DataFrame(Y).fillna(method=\"ffill\").fillna(0.0).to_numpy(dtype=np.float32)\n",
        "    return Y\n",
        "\n",
        "# Uncomment when you have the file:\n",
        "# Y = load_recovery_labels(RECOVERY_LABELS_CSV, nodes, y_col=\"recovery_index\")\n",
        "# print(\"Y:\", Y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) ST-GNN model (GCN + GRU) with quantile outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class STGNNQuantile(nn.Module):\n",
        "    def __init__(self, static_dim, dyn_dim, hidden=64, quantiles=(0.1, 0.5, 0.9)):\n",
        "        super().__init__()\n",
        "        self.quantiles = quantiles\n",
        "        self.gcn1 = GCNConv(static_dim + dyn_dim, hidden)\n",
        "        self.gcn2 = GCNConv(hidden, hidden)\n",
        "        self.gru = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
        "        self.head = nn.Linear(hidden, len(quantiles))\n",
        "\n",
        "    def forward(self, X_static, X_dyn, edge_index, edge_weight=None):\n",
        "        # X_static: (N,S), X_dyn: (T,N,F) for one ensemble\n",
        "        T, N, F = X_dyn.shape\n",
        "        h_seq = []\n",
        "        for t in range(T):\n",
        "            x = torch.cat([X_static, X_dyn[t]], dim=-1)\n",
        "            h = torch.relu(self.gcn1(x, edge_index, edge_weight))\n",
        "            h = torch.relu(self.gcn2(h, edge_index, edge_weight))\n",
        "            h_seq.append(h)\n",
        "        H = torch.stack(h_seq, dim=1)  # (N,T,H)\n",
        "        out, _ = self.gru(H)\n",
        "        q = self.head(out)             # (N,T,Q)\n",
        "        return q\n",
        "\n",
        "def quantile_loss(pred_q, y, quantiles=(0.1,0.5,0.9)):\n",
        "    losses = []\n",
        "    for qi, qv in enumerate(quantiles):\n",
        "        e = y - pred_q[..., qi]\n",
        "        losses.append(torch.max((qv - 1) * e, qv * e).mean())\n",
        "    return sum(losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Train across ensembles (uncertainty propagation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, X_static, X_dynamic, Y, edge_index, edge_weight=None, epochs=20):\n",
        "    device = next(model.parameters()).device\n",
        "    Xs = torch.tensor(X_static, dtype=torch.float32, device=device)\n",
        "    edge_index_t = torch.tensor(edge_index, dtype=torch.long, device=device)\n",
        "    ew = torch.tensor(edge_weight, dtype=torch.float32, device=device) if edge_weight is not None else None\n",
        "\n",
        "    Yt = torch.tensor(Y, dtype=torch.float32, device=device).transpose(0, 1)  # (N,T)\n",
        "    T, N, F, E = X_dynamic.shape\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total = 0.0\n",
        "        for e in range(E):\n",
        "            Xd = torch.tensor(X_dynamic[..., e], dtype=torch.float32, device=device)  # (T,N,F)\n",
        "            q = model(Xs, Xd, edge_index_t, ew)  # (N,T,Q)\n",
        "            total += quantile_loss(q, Yt, model.quantiles)\n",
        "\n",
        "        loss = total / E\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"epoch {ep+1}/{epochs} loss={loss.item():.4f}\")\n",
        "\n",
        "# Example (uncomment after loading X_dynamic and Y):\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = STGNNQuantile(static_dim=X_static.shape[1], dyn_dim=X_dynamic.shape[2], hidden=64).to(device)\n",
        "# optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# train(model, optim, X_static, X_dynamic, Y, edge_index, edge_weight, epochs=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Inference: mean/std + p10/p90 across ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_distribution(model, X_static, X_dynamic, edge_index, edge_weight=None):\n",
        "    device = next(model.parameters()).device\n",
        "    Xs = torch.tensor(X_static, dtype=torch.float32, device=device)\n",
        "    edge_index_t = torch.tensor(edge_index, dtype=torch.long, device=device)\n",
        "    ew = torch.tensor(edge_weight, dtype=torch.float32, device=device) if edge_weight is not None else None\n",
        "\n",
        "    T, N, F, E = X_dynamic.shape\n",
        "    preds = []\n",
        "    for e in range(E):\n",
        "        Xd = torch.tensor(X_dynamic[..., e], dtype=torch.float32, device=device)  # (T,N,F)\n",
        "        q = model(Xs, Xd, edge_index_t, ew)  # (N,T,Q)\n",
        "        preds.append(q.cpu().numpy())\n",
        "\n",
        "    preds = np.stack(preds, axis=0)  # (E,N,T,Q)\n",
        "    q50 = preds[..., 1]              # (E,N,T)\n",
        "\n",
        "    mean = q50.mean(axis=0)          # (N,T)\n",
        "    std  = q50.std(axis=0)           # (N,T)\n",
        "    p10  = np.quantile(q50, 0.10, axis=0)\n",
        "    p90  = np.quantile(q50, 0.90, axis=0)\n",
        "\n",
        "    return {\"mean\": mean, \"std\": std, \"p10\": p10, \"p90\": p90, \"raw\": preds}\n",
        "\n",
        "# Example:\n",
        "# dist = predict_distribution(model, X_static, X_dynamic, edge_index, edge_weight)\n",
        "# dist[\"mean\"].shape, dist[\"std\"].shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Planning baseline: allocate resources by risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_resources(nodes_df, recovery_mean, vulnerability_col=\"SVI\", budget=100, horizon_t=0):\n",
        "    N = len(nodes_df)\n",
        "    svi = pd.to_numeric(nodes_df.get(vulnerability_col, 0.0), errors=\"coerce\").fillna(0.0).to_numpy()\n",
        "\n",
        "    next_step = recovery_mean[:, horizon_t]  # (N,)\n",
        "    risk = svi * (1.0 - next_step)\n",
        "\n",
        "    risk = np.maximum(risk, 0.0)\n",
        "    alloc = np.zeros(N, dtype=np.float32) if risk.sum() == 0 else budget * (risk / risk.sum())\n",
        "\n",
        "    out = nodes_df[[\"id\",\"kind\",\"lat\",\"lon\"]].copy()\n",
        "    out[\"risk_score\"] = risk\n",
        "    out[\"resource_alloc\"] = alloc\n",
        "    return out.sort_values(\"risk_score\", ascending=False)\n",
        "\n",
        "# Example:\n",
        "# plan_df = plan_resources(nodes, dist[\"mean\"], vulnerability_col=\"SVI\", budget=100, horizon_t=0)\n",
        "# plan_df.head(20)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
